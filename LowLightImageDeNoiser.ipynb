{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-16T05:52:10.560201Z","iopub.status.busy":"2024-06-16T05:52:10.559806Z","iopub.status.idle":"2024-06-16T05:52:10.566507Z","shell.execute_reply":"2024-06-16T05:52:10.565428Z","shell.execute_reply.started":"2024-06-16T05:52:10.560173Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","import torch\n","import torch.nn as nn\n","import os\n","import glob\n","import shutil\n","from PIL import Image\n","import torch.nn.functional as F\n","import os\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:11.499682Z","iopub.status.busy":"2024-06-16T05:52:11.499100Z","iopub.status.idle":"2024-06-16T05:52:11.606940Z","shell.execute_reply":"2024-06-16T05:52:11.605732Z","shell.execute_reply.started":"2024-06-16T05:52:11.499651Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 5, 400, 592])\n"]}],"source":["# SCPA Block\n","class SCPA(nn.Module):\n","    def __init__(self,in_channels,out_channels):\n","        super(SCPA,self).__init__()\n","        self.conv1_branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv2_branch1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.conv1_branch2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv2_branch2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.conv3_branch2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.conv4_branch2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.final_conv =    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","    def forward(self,x):\n","        # branch 1\n","        branch1 = self.conv1_branch1(x)\n","        branch1 = self.conv2_branch1(branch1)\n","        # branch 2\n","        branch2 = self.conv1_branch2(x)\n","        branch2a = self.conv2_branch2(branch2)\n","        branch2a = self.sigmoid(branch2a)\n","        branch2b = self.conv3_branch2(branch2)\n","        branch2 = branch2a*branch2b\n","        branch2 = self.conv4_branch2(branch2)\n","        \n","        #combining branch 1 and branch 2\n","        output = branch2 + branch1\n","        \n","        #final convolutional layer and add it to the orignal input\n","        final_conv = self.final_conv(output)\n","        SCPA_output = final_conv + x\n","        return SCPA_output\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self,in_channels,out_channels,kernel_size=3,padding=1):\n","        super(ConvBlock, self).__init__()\n","        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding)\n","        self.bn=nn.BatchNorm2d(out_channels)\n","        self.relu=nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","\n","# CoordConv Bloack\n","class CoordConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(CoordConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels+2, out_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        batch_size, _, height, width = x.size()\n","        xx = torch.arange(width).repeat(height, 1)\n","        yy = torch.arange(height).view(-1, 1).repeat(1, width)\n","        xx = xx.float() / (width - 1)\n","        yy = yy.float() / (height - 1)\n","        xx = xx.repeat(batch_size, 1, 1).unsqueeze(1)\n","        yy = yy.repeat(batch_size, 1, 1).unsqueeze(1)\n","        if x.is_cuda:\n","            xx = xx.cuda()\n","            yy = yy.cuda()\n","        x = torch.cat([x, xx, yy], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","if __name__ == \"__main__\":\n","    model = CoordConv(in_channels=3, out_channels=5)\n","    input_tensor = torch.randn(1, 3, 400, 592)\n","    output_tensor = model(input_tensor)\n","    print(output_tensor.shape)\n","    \n","class AttentionBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(AttentionBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        #Calculate attention weights\n","        att_weights = self.conv1(x)\n","        att_weights = self.sigmoid(att_weights)\n","        #Apply attention to input features\n","        output = x * att_weights\n","        return output\n","\n","#inverse residual block\n","class inverted_residual_block(nn.Module):\n","    def __init__(self,in_channels,out_channels,expansion_factor,stride): #3361\n","        super(inverted_residual_block,self).__init__()\n","        self.stride=stride\n","        hidden_dim=in_channels*expansion_factor\n","        self.use_residual=self.stride==1 and in_channels==out_channels\n","\n","        layers=[]\n","        if expansion_factor!= 1:\n","            layers.append(nn.Conv2d(in_channels,hidden_dim,kernel_size=1,bias=False))\n","            layers.append(nn.BatchNorm2d(hidden_dim))\n","            layers.append(nn.ReLU6(inplace=True))\n","\n","        layers.append(nn.Conv2d(hidden_dim,hidden_dim,kernel_size=3,stride=stride,padding=1,groups=hidden_dim,bias=False))\n","        layers.append(nn.BatchNorm2d(hidden_dim))\n","        layers.append(nn.ReLU6(inplace=True))\n","\n","        layers.append(nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False))\n","        layers.append(nn.BatchNorm2d(out_channels))\n","\n","        self.conv=nn.Sequential(*layers)\n","\n","    def forward(self,x):\n","        if self.use_residual:\n","            return x+self.conv(x)\n","        else:\n","            return self.conv(x)\n","    \n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        return out + residual\n","\n","class DenseBlock(nn.Module):\n","    def __init__(self, in_channels, growth_rate=32, num_layers=4):\n","        super(DenseBlock, self).__init__()\n","        self.layers = nn.ModuleList()\n","        for i in range(num_layers):\n","            self.layers.append(nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1))\n","            self.layers.append(nn.BatchNorm2d(growth_rate))\n","            self.layers.append(nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        features = [x]\n","        for i in range(0, len(self.layers), 3):\n","            out = self.layers[i](torch.cat(features, dim=1))\n","            out = self.layers[i + 1](out)\n","            out = self.layers[i + 2](out)\n","            features.append(out)\n","        return torch.cat(features, dim=1)\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=3):\n","        super(SpatialAttention, self).__init__()\n","        assert kernel_size in (3, 7)\n","        padding = kernel_size // 2\n","        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv(x)\n","        return x * self.sigmoid(x)\n","\n","class ResidualDenseAttention(nn.Module):\n","    def __init__(self, in_channels, growth_rate=32, num_layers=4, kernel_size=7):\n","        super(ResidualDenseAttention, self).__init__()\n","        self.residual_block = ResidualBlock(in_channels)\n","        self.dense_block = DenseBlock(in_channels, growth_rate, num_layers)\n","        self.attention_block = SpatialAttention(kernel_size)\n","\n","    def forward(self, x):\n","        residual_out = self.residual_block(x)\n","        dense_out = self.dense_block(residual_out)\n","        attention_out = self.attention_block(dense_out)\n","        return attention_out + x\n","\n","class downsampling_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(downsampling_block, self).__init__()\n","        self.conv=nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn=nn.BatchNorm2d(out_channels)\n","        self.relu=nn.ReLU(inplace=True)\n","        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        x=self.conv(x)\n","        x=self.bn(x)\n","        x=self.relu(x)\n","        x=self.pool(x)\n","        return x\n","    \n","class upsampling_block(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(upsampling_block, self).__init__()\n","        self.upconv=nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","        self.conv=nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn=nn.BatchNorm2d(out_channels)\n","        self.relu=nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x=self.upconv(x)\n","        x=self.conv(x)\n","        x=self.bn(x)\n","        x=self.relu(x)\n","        return x\n","\n","class SCPA_Branch(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(SCPA_Branch,self).__init__()\n","        self.coord_layer = CoordConv(in_channels =3,out_channels = 5)\n","        self.SCPA_1 = SCPA(in_channels = 5, out_channels =5)\n","        self.SCPA_2 = SCPA(in_channels = 5, out_channels =5)\n","        self.SCPA_3 = SCPA(in_channels = 5, out_channels =5)\n","        self.SCPA_4 = SCPA(in_channels = 5, out_channels =5)\n","        self.SCPA_5 = SCPA(in_channels = 5, out_channels =5)\n","        self.conv_layer =nn.Conv2d(in_channels=5,out_channels =3,kernel_size = 3,padding =1)\n","        \n","    def forward(self,x):\n","        x = self.coord_layer(x)\n","        x = self.SCPA_1(x)\n","        x = self.SCPA_2(x)\n","        x = self.SCPA_3(x)\n","        x = self.SCPA_4(x)\n","        x = self.SCPA_5(x)\n","        x = self.conv_layer(x)\n","        return x \n","\n","# Denoising branch\n","'''\n","convolutional block -> 4 inv residual block -> attention block -> convolution block \n","'''\n","class DenoiseBranch(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(DenoiseBranch,self).__init__()\n","        self.conv_1 = ConvBlock(3,3)\n","        self.inv_1 = inverted_residual_block(3,3,6,1)\n","        self.inv_2 = inverted_residual_block(3,3,6,1)\n","        self.inv_3 = inverted_residual_block(3,3,6,1)\n","        self.inv_4 = inverted_residual_block(3,3,6,1)\n","        self.attention = AttentionBlock(in_channels = 3)\n","        self.conv_2 = ConvBlock(3,3)\n","    \n","    def forward(self,x):\n","        x = self.conv_1(x)\n","        x = self.inv_1(x)\n","        x = self.inv_2(x)\n","        x = self.inv_3(x)\n","        x = self.inv_4(x)\n","        x = self.attention(x)\n","        x = self.conv_2(x)\n","        return x\n","    \n","class Encoder(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(Encoder,self).__init__()\n","        self.RDA_Block1= ResidualDenseAttention(in_channels=in_channel, growth_rate=32, num_layers=4, kernel_size=7)\n","        self.RDA_Block2= ResidualDenseAttention(in_channels=in_channel, growth_rate=32, num_layers=4, kernel_size=7)\n","        self.Downsampler = downsampling_block(in_channel,out_channel)\n","    def forward(self,x):\n","        x = self.RDA_Block1(x)\n","        x = self.RDA_Block2(x)\n","        x = self.Downsampler(x)\n","        return x\n","    \n","class Decoder(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(Decoder,self).__init__()\n","        self.RDA_Block1= ResidualDenseAttention(in_channels=in_channel, growth_rate=32, num_layers=4, kernel_size=7)\n","        self.RDA_Block2= ResidualDenseAttention(in_channels=in_channel, growth_rate=32, num_layers=4, kernel_size=7)\n","        self.Upsampler = upsampling_block(in_channel,out_channel)\n","    def forward(self,x):\n","        x = self.RDA_Block1(x)\n","        x = self.RDA_Block2(x)\n","        x = self.Upsampler(x)\n","        return x\n","    \n","class FinalAutoencoder(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(FinalAutoencoder,self).__init__()\n","        self.encoder1 = Encoder(in_channel,32)\n","        self.encoder2 = Encoder(32,64)\n","        self.encoder3 = Encoder(64,128)\n","        self.encoder4 = Encoder(128,256)\n","        self.decoder1 = Decoder(256,128)\n","        self.decoder2 = Decoder(128+128,64)\n","        self.decoder3 = Decoder(64+64,32)\n","        self.decoder4 = Decoder(32+32,out_channel)\n","\n","    def forward(self,x):\n","        x1 = self.encoder1(x)\n","        x2 = self.encoder2(x1)\n","        x3 = self.encoder3(x2)\n","        x4 = self.encoder4(x3)\n","        x5 = self.decoder1(x4)\n","        x6 = self.decoder2(torch.cat([x5,x3],1))\n","        x7 = self.decoder3(torch.cat([x6,x2],1))\n","        x8 = self.decoder4(torch.cat([x7,x1],1))\n","        return x8\n","    \n","class LowLightModel(nn.Module):\n","    def __init__(self,in_channel=3,out_channel=3):\n","        super(LowLightModel,self).__init__()\n","        self.SCPA_branch = SCPA_Branch()\n","        self.Denoiser = DenoiseBranch()\n","        self.AutoEncoder = FinalAutoencoder()\n","        self.Conv = ConvBlock(3,3)\n","        \n","    def forward(self,x):\n","        denoised = self.Denoiser(x)\n","        SCPA = self.SCPA_branch(x)\n","        Auto_input = SCPA + x\n","        Auto_output = self.AutoEncoder(Auto_input)\n","        output = self.Conv(Auto_output)\n","        final = output + denoised\n","        return final"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:14.560514Z","iopub.status.busy":"2024-06-16T05:52:14.560124Z","iopub.status.idle":"2024-06-16T05:52:14.569112Z","shell.execute_reply":"2024-06-16T05:52:14.568069Z","shell.execute_reply.started":"2024-06-16T05:52:14.560485Z"},"trusted":true},"outputs":[],"source":["class LowLightDataset(Dataset):\n","    def __init__(self, low_img_dir, high_img_dir, transform=None):\n","        self.low_img_dir = low_img_dir\n","        self.high_img_dir = high_img_dir\n","        self.low_images = sorted(os.listdir(low_img_dir))\n","        self.high_images = sorted(os.listdir(high_img_dir))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.low_images)\n","\n","    def __getitem__(self, idx):\n","        low_img_path = os.path.join(self.low_img_dir, self.low_images[idx])\n","        high_img_path = os.path.join(self.high_img_dir, self.high_images[idx])\n","        low_image = Image.open(low_img_path).convert(\"RGB\")\n","        high_image = Image.open(high_img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            low_image = self.transform(low_image)\n","            high_image = self.transform(high_image)\n","\n","        return low_image, high_image\n","\n","transform = transforms.Compose([\n","    transforms.Resize((400, 592)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:17.471842Z","iopub.status.busy":"2024-06-16T05:52:17.471496Z","iopub.status.idle":"2024-06-16T05:52:17.476997Z","shell.execute_reply":"2024-06-16T05:52:17.476077Z","shell.execute_reply.started":"2024-06-16T05:52:17.471814Z"},"trusted":true},"outputs":[],"source":["def psnr(img1, img2):\n","    mse = F.mse_loss(img1, img2)\n","    if mse == 0:\n","        return 100\n","    pixel_max = 1.0\n","    return 20 * torch.log10(pixel_max / torch.sqrt(mse))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:18.855136Z","iopub.status.busy":"2024-06-16T05:52:18.854269Z","iopub.status.idle":"2024-06-16T05:52:19.705053Z","shell.execute_reply":"2024-06-16T05:52:19.704260Z","shell.execute_reply.started":"2024-06-16T05:52:18.855104Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","train_low_dir = '/kaggle/input/dataset00/augmented_Train/augmented/low'\n","train_high_dir = '/kaggle/input/dataset00/augmented_Train/augmented/high'\n","val_low_dir = '/kaggle/input/dataset00/augmented_Train/val/low'\n","val_high_dir = '/kaggle/input/dataset00/augmented_Train/val/high'\n","\n","train_dataset = LowLightDataset(train_low_dir, train_high_dir, transform=transform)\n","val_dataset = LowLightDataset(val_low_dir, val_high_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:22.961023Z","iopub.status.busy":"2024-06-16T05:52:22.960651Z","iopub.status.idle":"2024-06-16T05:52:22.980470Z","shell.execute_reply":"2024-06-16T05:52:22.979462Z","shell.execute_reply.started":"2024-06-16T05:52:22.960995Z"},"trusted":true},"outputs":[],"source":["def gaussian(window_size, sigma):\n","    gauss = torch.tensor([torch.exp(torch.tensor(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2))) for x in range(window_size)], dtype=torch.float32)\n","    return gauss / gauss.sum()\n","\n","def create_window(window_size, channel):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n","    return window\n","\n","def ssim(img1, img2, window_size=11, size_average=True):\n","    (_, channel, _, _) = img1.size()\n","    window = create_window(window_size, channel).to(img1.device)\n","\n","    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n","    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1 * mu2\n","\n","    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n","    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n","\n","    C1 = 0.01 ** 2\n","    C2 = 0.03 ** 2\n","\n","    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n","\n","    return ssim_map.mean() if size_average else ssim_map.mean(1).mean(1).mean(1)\n","\n","class SSIMLoss(nn.Module):\n","    def __init__(self, window_size=11, size_average=True):\n","        super(SSIMLoss, self).__init__()\n","        self.window_size = window_size\n","        self.size_average = size_average\n","\n","    def forward(self, img1, img2):\n","        return 1 - ssim(img1, img2, self.window_size, self.size_average)\n","\n","# Combined Loss Function\n","class CombinedLoss(nn.Module):\n","    def __init__(self):\n","        super(CombinedLoss, self).__init__()\n","        self.ssim_loss = SSIMLoss()\n","        self.l1_loss = nn.L1Loss()\n","\n","    def forward(self, output, target):\n","        # L1 loss\n","        l1_loss = self.l1_loss(output, target)\n","        \n","        # SSIM loss\n","        ssim_loss = self.ssim_loss(output, target)\n","        \n","        # Gradient loss\n","        grad_loss = self.gradient_loss(output, target)\n","        \n","        # Combined loss\n","        total_loss = 0.1 * ssim_loss + l1_loss + grad_loss\n","        \n","        return total_loss\n","\n","    def gradient_loss(self, output, target):\n","        # Compute gradients\n","        output_grad_x = torch.abs(output[:, :, :, :-1] - output[:, :, :, 1:])\n","        output_grad_y = torch.abs(output[:, :, :-1, :] - output[:, :, 1:, :])\n","        target_grad_x = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])\n","        target_grad_y = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])\n","        \n","        # Compute gradient loss\n","        grad_loss_x = F.l1_loss(output_grad_x, target_grad_x)\n","        grad_loss_y = F.l1_loss(output_grad_y, target_grad_y)\n","        \n","        return grad_loss_x + grad_loss_y\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:24.958695Z","iopub.status.busy":"2024-06-16T05:52:24.958047Z","iopub.status.idle":"2024-06-16T05:52:25.847084Z","shell.execute_reply":"2024-06-16T05:52:25.846122Z","shell.execute_reply.started":"2024-06-16T05:52:24.958659Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model = LowLightModel()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","criterion = CombinedLoss()\n","model.load_state_dict(torch.load('model_weights.pth'))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-15T20:14:01.574092Z","iopub.status.busy":"2024-06-15T20:14:01.573396Z","iopub.status.idle":"2024-06-15T20:39:23.272852Z","shell.execute_reply":"2024-06-15T20:39:23.271948Z","shell.execute_reply.started":"2024-06-15T20:14:01.574062Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                       \r"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/5], Train Loss: 0.1361, Train PSNR: 24.16 dB\n"]},{"name":"stderr","output_type":"stream","text":["                                                                       \r"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/5], Train Loss: 0.1316, Train PSNR: 24.55 dB\n","Model saved at epoch 2\n"]},{"name":"stderr","output_type":"stream","text":["                                                                       \r"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/5], Train Loss: 0.1321, Train PSNR: 24.47 dB\n"]},{"name":"stderr","output_type":"stream","text":["                                                                       \r"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/5], Train Loss: 0.1295, Train PSNR: 24.72 dB\n","Model saved at epoch 4\n"]},{"name":"stderr","output_type":"stream","text":["                                                                       "]},{"name":"stdout","output_type":"stream","text":["Epoch [5/5], Train Loss: 0.1311, Train PSNR: 24.64 dB\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","new_epochs = 5\n","start_epoch = 0\n","total_epochs = start_epoch + new_epochs\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(start_epoch, total_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    train_psnr = 0.0\n","    \n","    for low_img, high_img in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{total_epochs}] Training\", leave=False):\n","        low_img, high_img = low_img.to(device), high_img.to(device)\n","        \n","        output = model(low_img)\n","        loss = criterion(output, high_img)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step() \n","        train_loss += loss.item() \n","        batch_psnr = psnr(output, high_img).item()\n","        train_psnr += batch_psnr\n","    \n","    model.eval()\n","    train_loss /= len(train_loader)\n","    train_psnr /= len(train_loader)\n","    \n","    print(f\"Epoch [{epoch+1}/{total_epochs}], Train Loss: {train_loss:.4f}, Train PSNR: {train_psnr:.2f} dB\")\n","    if (epoch + 1) % 2 == 0:\n","        model_path = f'model_epoch_{epoch+1}.pth'\n","        torch.save(model.state_dict(), model_path)\n","        print(f\"Model saved at epoch {epoch+1}\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T05:52:56.395403Z","iopub.status.busy":"2024-06-16T05:52:56.394790Z","iopub.status.idle":"2024-06-16T05:53:05.986686Z","shell.execute_reply":"2024-06-16T05:53:05.985651Z","shell.execute_reply.started":"2024-06-16T05:52:56.395360Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average PSNR: 24.58 dB\n"]}],"source":["low_dir = '/kaggle/input/dataset00/augmented_Train/val/low'\n","high_dir = '/kaggle/input/dataset00/augmented_Train/val/high'\n","\n","# List all images in the low light directory\n","low_images = os.listdir(low_dir)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","psnr_values = []\n","\n","# Iterate over all images in the validation set\n","for image_name in low_images:\n","    input_image_path = os.path.join(low_dir, image_name)\n","    high_image_path = os.path.join(high_dir, image_name)\n","    \n","    # Open and preprocess the input image\n","    input_image = Image.open(input_image_path).convert('RGB')\n","    input_tensor = transform(input_image).unsqueeze(0).to(device)\n","    \n","    with torch.no_grad():\n","        enhanced_tensor = model(input_tensor).cpu()\n","\n","    # Read and convert the original high light image\n","    original_image = cv2.imread(high_image_path)\n","    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","    \n","    # Convert original image to tensor\n","    original_tensor = transform(Image.fromarray(original_image_rgb)).unsqueeze(0)\n","    \n","    # Calculate PSNR and store the value\n","    psnr_value = psnr(original_tensor, enhanced_tensor)\n","    psnr_values.append(psnr_value.item())\n","\n","# Calculate the average PSNR\n","average_psnr = sum(psnr_values) / len(psnr_values)\n","print(f'Average PSNR: {average_psnr:.2f} dB')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","model.eval()\n","transform = transforms.Compose([\n","    transforms.Resize((400, 592)),\n","    transforms.ToTensor() \n","])\n","model.eval()\n","\n","low_dir = './test/low'\n","high_dir = './test/predicted'\n","\n","low_images = os.listdir(low_dir)\n","\n","random_images = random.sample(low_images,5)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","psnr_values = []\n","\n","for image_name in random_images:\n","    input_image_path = os.path.join(low_dir, image_name)\n","    high_image_path = os.path.join(high_dir, image_name)\n","    input_image = Image.open(input_image_path).convert('RGB')\n","    input_tensor = transform(input_image).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        enhanced_tensor = model(input_tensor).cpu()\n","\n","    enhanced_image = transforms.ToPILImage()(enhanced_tensor.squeeze())\n","\n","    original_image = cv2.imread(high_image_path)\n","    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","    \n","    fig, axs = plt.subplots(1, 3, figsize=(10, 5))  \n","    axs[0].imshow(input_image)\n","    axs[0].set_title('Low Light')\n","    axs[0].axis('off')\n","    \n","    axs[1].imshow(enhanced_image)\n","    axs[1].set_title('Enhanced')\n","    axs[1].axis('off')\n","    \n","    axs[2].imshow(original_image_rgb)\n","    axs[2].set_title('Ground Truth')\n","    axs[2].axis('off')\n","    \n","    plt.show()\n","    \n","    original_tensor = transform(Image.fromarray(original_image_rgb)).unsqueeze(0)\n","    psnr_value = psnr(original_tensor, enhanced_tensor)\n","    psnr_values.append(psnr_value)\n","    print(f'PSNR: {psnr_value:.2f} dB')\n","average_psnr = sum(psnr_values) / len(psnr_values)\n","print(f'Average PSNR: {average_psnr:.2f} dB')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import cv2\n","from torchmetrics.functional import peak_signal_noise_ratio as psnr\n","\n","model.eval()\n","transform = transforms.Compose([\n","    transforms.Resize((400, 592)),\n","    transforms.ToTensor() \n","])\n","low_dir = './test/low'\n","high_dir = './test/predicted'\n","\n","low_images = os.listdir(low_dir)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","psnr_values = []\n","\n","for image_name in low_images:\n","    input_image_path = os.path.join(low_dir, image_name)\n","    high_image_path = os.path.join(high_dir, image_name)\n","    \n","    input_image = Image.open(input_image_path).convert('RGB')\n","    input_tensor = transform(input_image).unsqueeze(0).to(device)\n","    \n","    with torch.no_grad():\n","        enhanced_tensor = model(input_tensor).cpu()\n","\n","    original_image = cv2.imread(high_image_path)\n","    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","    \n","    original_tensor = transform(Image.fromarray(original_image_rgb)).unsqueeze(0)\n","    \n","    psnr_value = psnr(original_tensor, enhanced_tensor)\n","    psnr_values.append(psnr_value.item())\n","\n","average_psnr = sum(psnr_values) / len(psnr_values)\n","print(f'Average PSNR: {average_psnr:.2f} dB')\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5127453,"sourceId":8574883,"sourceType":"datasetVersion"},{"datasetId":5202472,"sourceId":8678843,"sourceType":"datasetVersion"},{"datasetId":5209220,"sourceId":8688093,"sourceType":"datasetVersion"},{"datasetId":5210527,"sourceId":8689835,"sourceType":"datasetVersion"},{"datasetId":5212435,"sourceId":8692366,"sourceType":"datasetVersion"},{"datasetId":5214981,"sourceId":8695986,"sourceType":"datasetVersion"},{"datasetId":5216490,"sourceId":8698041,"sourceType":"datasetVersion"},{"datasetId":5218182,"sourceId":8700447,"sourceType":"datasetVersion"},{"datasetId":5219359,"sourceId":8702275,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
